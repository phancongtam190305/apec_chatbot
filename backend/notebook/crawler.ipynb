{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6677b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re # Để sử dụng biểu thức chính quy cho phân trang\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b514210",
   "metadata": {},
   "source": [
    "# Crawling raw HTML text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80a2048f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thư mục 'data/crawled_raw_html' đã sẵn sàng để lưu trữ HTML.\n",
      "\n",
      "--- Đang xử lý: Information_of_Apec từ https://apec2025.kr/?menuno=89 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Information_of_Apec.html\n",
      "\n",
      "--- Đang xử lý: Introduction_About_Apec_Korea_2025 từ https://apec2025.kr/?menuno=91 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Introduction_About_Apec_Korea_2025.html\n",
      "\n",
      "--- Đang xử lý: Emblem_and_Theme từ https://apec2025.kr/?menuno=92 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Emblem_and_Theme.html\n",
      "\n",
      "--- Đang xử lý: Meetings từ https://apec2025.kr/?menuno=93 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Meetings.html\n",
      "\n",
      "--- Đang xử lý: Side_Events từ https://apec2025.kr/?menuno=94 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Side_Events.html\n",
      "\n",
      "--- Đang xử lý: Documents_HRDDM từ https://apec2025.kr/?menuno=148 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Documents_HRDDM.html\n",
      "\n",
      "--- Đang xử lý: Documents_AEMM từ http://apec2025.kr/?menuno=149 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Documents_AEMM.html\n",
      "\n",
      "--- Đang xử lý: Documents_MRT từ https://apec2025.kr/?menuno=150 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Documents_MRT.html\n",
      "\n",
      "--- Đang xử lý: Notices từ https://apec2025.kr/?menuno=15 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Notices.html\n",
      "\n",
      "--- Đang xử lý: Press_Release từ https://apec2025.kr/?menuno=16 ---\n",
      "  > Đang tải trang 1 của Press_Release từ https://apec2025.kr/?menuno=16...\n",
      "  > Cập nhật tổng số trang cho Press_Release thành: 5\n",
      "  + Đã thêm 5 bài viết từ trang 1 (đã bọc <article>) vào file tổng.\n",
      "  > Đang tải trang 2 của Press_Release từ https://apec2025.kr/?menuno=16&pageNum=2...\n",
      "  + Đã thêm 5 bài viết từ trang 2 (đã bọc <article>) vào file tổng.\n",
      "  > Đang tải trang 3 của Press_Release từ https://apec2025.kr/?menuno=16&pageNum=3...\n",
      "  + Đã thêm 5 bài viết từ trang 3 (đã bọc <article>) vào file tổng.\n",
      "  > Đang tải trang 4 của Press_Release từ https://apec2025.kr/?menuno=16&pageNum=4...\n",
      "  + Đã thêm 5 bài viết từ trang 4 (đã bọc <article>) vào file tổng.\n",
      "  > Đang tải trang 5 của Press_Release từ https://apec2025.kr/?menuno=16&pageNum=5...\n",
      "  + Đã thêm 3 bài viết từ trang 5 (đã bọc <article>) vào file tổng.\n",
      "✅ Đã lưu tất cả nội dung Press Release vào: data/crawled_raw_html\\Press_Release_combined.html\n",
      "\n",
      "--- Đang xử lý: Korea_in_Brief từ https://apec2025.kr/?menuno=18 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Korea_in_Brief.html\n",
      "\n",
      "--- Đang xử lý: Practical_Information từ https://apec2025.kr/?menuno=22 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Practical_Information.html\n",
      "\n",
      "--- Đang xử lý: About_Gyeongju từ https://apec2025.kr/?menuno=102 ---\n",
      "  Đã lưu: data/crawled_raw_html\\About_Gyeongju.html\n",
      "\n",
      "--- Đang xử lý: Transportation_of_Gyeongju từ https://apec2025.kr/?menuno=137 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Transportation_of_Gyeongju.html\n",
      "\n",
      "--- Đang xử lý: Heritage_Gyeongju từ https://apec2025.kr/?menuno=108 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Heritage_Gyeongju.html\n",
      "\n",
      "--- Đang xử lý: Attraction_of_Gyeongju từ https://apec2025.kr/?menuno=138 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Attraction_of_Gyeongju.html\n",
      "\n",
      "--- Đang xử lý: About_Jeju từ https://apec2025.kr/?menuno=103 ---\n",
      "  Đã lưu: data/crawled_raw_html\\About_Jeju.html\n",
      "\n",
      "--- Đang xử lý: Transportation_Jeju từ https://apec2025.kr/?menuno=141 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Transportation_Jeju.html\n",
      "\n",
      "--- Đang xử lý: Nature_Culture_Jeju từ https://apec2025.kr/?menuno=114 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Nature_Culture_Jeju.html\n",
      "\n",
      "--- Đang xử lý: Themed_Travel_Jeju từ https://apec2025.kr/?menuno=115 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Themed_Travel_Jeju.html\n",
      "\n",
      "--- Đang xử lý: About_Incheon từ https://apec2025.kr/?menuno=104 ---\n",
      "  Đã lưu: data/crawled_raw_html\\About_Incheon.html\n",
      "\n",
      "--- Đang xử lý: Attractions_Incheon từ https://apec2025.kr/?menuno=117 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Attractions_Incheon.html\n",
      "\n",
      "--- Đang xử lý: Local_Eateries_Incheon từ https://apec2025.kr/?menuno=118 ---\n",
      "  Đã lưu: data/crawled_raw_html\\Local_Eateries_Incheon.html\n",
      "\n",
      "--- Đang xử lý: About_Busan từ https://apec2025.kr/?menuno=106 ---\n",
      "  Đã lưu: data/crawled_raw_html\\About_Busan.html\n",
      "\n",
      "--- Đang xử lý: About_Seoul từ https://apec2025.kr/?menuno=24 ---\n",
      "  Đã lưu: data/crawled_raw_html\\About_Seoul.html\n",
      "\n",
      "--- ✅ Hoàn tất quá trình crawl và lưu HTML ---\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import json # Vẫn giữ để nếu sau này muốn tùy chỉnh xuất riêng JSON cho PR\n",
    "import uuid # Cần thiết cho ID duy nhất\n",
    "\n",
    "# Hàm hỗ trợ để lấy số trang tối đa từ phần phân trang (không thay đổi)\n",
    "def get_max_page_number(soup):\n",
    "    max_page = 1\n",
    "    pagination_links = soup.select('.webtong-paging #numbering a')\n",
    "    current_page_elem = soup.select_one('.webtong-paging #numbering em')\n",
    "    if current_page_elem:\n",
    "        try:\n",
    "            max_page = max(max_page, int(current_page_elem.get_text(strip=True)))\n",
    "        except ValueError:\n",
    "            pass\n",
    "    for link in pagination_links:\n",
    "        if link.has_attr('onclick'):\n",
    "            match = re.search(r'submitForm\\(this,\\s*\"list\",\\s*(\\d+)\\);', link['onclick'])\n",
    "            if match:\n",
    "                page_num = int(match.group(1))\n",
    "                max_page = max(max_page, page_num)\n",
    "    last_btn = soup.select_one('.webtong-paging .last')\n",
    "    if last_btn and last_btn.has_attr('onclick'):\n",
    "        match = re.search(r'submitForm\\(this,\\s*\"list\",\\s*(\\d+)\\);', last_btn['onclick'])\n",
    "        if match:\n",
    "            max_page = max(max_page, int(match.group(1)))\n",
    "    return max_page\n",
    "\n",
    "\n",
    "def crawl_and_save_html(urls_to_crawl, output_dir=\"data/crawled_raw_html\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"Thư mục '{output_dir}' đã sẵn sàng để lưu trữ HTML.\")\n",
    "\n",
    "    for name, base_url in urls_to_crawl.items():\n",
    "        print(f\"\\n--- Đang xử lý: {name} từ {base_url} ---\")\n",
    "        \n",
    "        # Đường dẫn cho file HTML tổng hợp hoặc file HTML đơn\n",
    "        file_path_to_save = os.path.join(output_dir, f\"{name}.html\")\n",
    "        if name == \"Press_Release\":\n",
    "            file_path_to_save = os.path.join(output_dir, f\"{name}_combined.html\")\n",
    "\n",
    "        if os.path.exists(file_path_to_save):\n",
    "            print(f\"  File '{file_path_to_save}' đã tồn tại. Bỏ qua crawl cho {name}.\")\n",
    "            continue # Bỏ qua nếu file đã tồn tại\n",
    "\n",
    "        # Xử lý riêng cho Press Release (có phân trang)\n",
    "        if name == \"Press_Release\":\n",
    "            with open(file_path_to_save, \"w\", encoding=\"utf-8\") as combined_file:\n",
    "                # Bọc toàn bộ nội dung trong một thẻ <main> để UnstructuredHTMLLoader dễ nhận diện nội dung chính\n",
    "                combined_file.write(\"<!DOCTYPE html>\\n<html><head><meta charset='utf-8'></head><body>\\n\")\n",
    "                combined_file.write(\"<main>\\n\") # Thẻ <main> để bọc nội dung chính\n",
    "                \n",
    "                current_page = 1\n",
    "                max_page_found = 1 # Ban đầu giả định chỉ có 1 trang\n",
    "\n",
    "                while current_page <= max_page_found:\n",
    "                    url = f\"{base_url}&pageNum={current_page}\" if current_page > 1 else base_url\n",
    "                    print(f\"  > Đang tải trang {current_page} của {name} từ {url}...\")\n",
    "                    try:\n",
    "                        response = requests.get(url, timeout=20)\n",
    "                        response.raise_for_status()\n",
    "                        html_content = response.text\n",
    "                        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "                        # Cập nhật số trang tối đa (để biết khi nào dừng)\n",
    "                        new_max_page = get_max_page_number(soup)\n",
    "                        if new_max_page > max_page_found:\n",
    "                            max_page_found = new_max_page\n",
    "                            print(f\"  > Cập nhật tổng số trang cho {name} thành: {max_page_found}\")\n",
    "\n",
    "                        # Tìm phần chứa các bài viết (từng thẻ <li> của mỗi bài)\n",
    "                        article_list_items = soup.select('.board_list1 .event > li')\n",
    "                        \n",
    "                        if article_list_items:\n",
    "                            for item in article_list_items:\n",
    "                                # TRỌNG TÂM ĐIỀU CHỈNH: Bọc mỗi <li> trong thẻ <article>\n",
    "                                # UnstructuredHTMLLoader rất giỏi nhận diện <article> như một tài liệu độc lập\n",
    "                                combined_file.write(f\"<article data-source-url='{url}' data-article-id='{str(uuid.uuid4())}'>\\n\")\n",
    "                                combined_file.write(str(item) + \"\\n\")\n",
    "                                combined_file.write(\"</article>\\n\")\n",
    "                            print(f\"  + Đã thêm {len(article_list_items)} bài viết từ trang {current_page} (đã bọc <article>) vào file tổng.\")\n",
    "                        else:\n",
    "                            print(f\"  ! Không tìm thấy bài viết nào trên trang {current_page}. Dừng crawl {name}.\")\n",
    "                            break\n",
    "\n",
    "                        current_page += 1\n",
    "\n",
    "                        # Điều kiện dừng vòng lặp\n",
    "                        if current_page > max_page_found:\n",
    "                            break\n",
    "\n",
    "                    except requests.exceptions.RequestException as e:\n",
    "                        print(f\"  ! Lỗi khi tải trang {url}: {e}. Dừng crawl {name}.\")\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ! Lỗi xử lý trang {url}: {e}. Dừng crawl {name}.\")\n",
    "                        break\n",
    "\n",
    "                combined_file.write(\"</main>\\n\") # Đóng thẻ <main>\n",
    "                combined_file.write(\"</body></html>\\n\")\n",
    "                print(f\"✅ Đã lưu tất cả nội dung Press Release vào: {file_path_to_save}\")\n",
    "\n",
    "        else: # Xử lý các trang không phân trang (HTML đơn)\n",
    "            try:\n",
    "                response = requests.get(base_url, timeout=20)\n",
    "                response.raise_for_status()\n",
    "                html_content = response.text\n",
    "\n",
    "                with open(file_path_to_save, \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(html_content)\n",
    "                print(f\"  Đã lưu: {file_path_to_save}\")\n",
    "\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"  ! Lỗi khi tải trang {base_url}: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ! Lỗi xử lý trang {base_url}: {e}\")\n",
    "\n",
    "    print(\"\\n--- ✅ Hoàn tất quá trình crawl và lưu HTML ---\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    urls_and_names = {\n",
    "        \"Information_of_Apec\": \"https://apec2025.kr/?menuno=89\",\n",
    "        \"Introduction_About_Apec_Korea_2025\": \"https://apec2025.kr/?menuno=91\",\n",
    "        \"Emblem_and_Theme\": \"https://apec2025.kr/?menuno=92\",\n",
    "        \"Meetings\": \"https://apec2025.kr/?menuno=93\",\n",
    "        \"Side_Events\": \"https://apec2025.kr/?menuno=94\",\n",
    "        \"Documents_HRDDM\": \"https://apec2025.kr/?menuno=148\",\n",
    "        \"Documents_AEMM\": \"http://apec2025.kr/?menuno=149\", \n",
    "        \"Documents_MRT\": \"https://apec2025.kr/?menuno=150\",\n",
    "        \"Notices\": \"https://apec2025.kr/?menuno=15\",\n",
    "        \"Press_Release\": \"https://apec2025.kr/?menuno=16\", # Trang này có phân trang\n",
    "        \"Korea_in_Brief\": \"https://apec2025.kr/?menuno=18\",\n",
    "        \"Practical_Information\": \"https://apec2025.kr/?menuno=22\",\n",
    "        \"About_Gyeongju\": \"https://apec2025.kr/?menuno=102\",\n",
    "        \"Transportation_of_Gyeongju\": \"https://apec2025.kr/?menuno=137\",\n",
    "        \"Heritage_Gyeongju\": \"https://apec2025.kr/?menuno=108\",\n",
    "        \"Attraction_of_Gyeongju\": \"https://apec2025.kr/?menuno=138\",\n",
    "        \"About_Jeju\": \"https://apec2025.kr/?menuno=103\",\n",
    "        \"Transportation_Jeju\": \"https://apec2025.kr/?menuno=141\",\n",
    "        \"Nature_Culture_Jeju\": \"https://apec2025.kr/?menuno=114\",\n",
    "        \"Themed_Travel_Jeju\": \"https://apec2025.kr/?menuno=115\",\n",
    "        \"About_Incheon\": \"https://apec2025.kr/?menuno=104\",\n",
    "        \"Attractions_Incheon\": \"https://apec2025.kr/?menuno=117\",\n",
    "        \"Local_Eateries_Incheon\": \"https://apec2025.kr/?menuno=118\",\n",
    "        \"About_Busan\": \"https://apec2025.kr/?menuno=106\",\n",
    "        \"About_Seoul\": \"https://apec2025.kr/?menuno=24\",\n",
    "    }\n",
    "    \n",
    "    crawl_and_save_html(urls_and_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd10c86",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450666b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1180980a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import uuid\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5cfd8048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tìm thấy 25 file HTML để xử lý.\n",
      "Đang xử lý file: About_Busan.html\n",
      "Đang xử lý file: About_Gyeongju.html\n",
      "Đang xử lý file: About_Incheon.html\n",
      "Đang xử lý file: About_Jeju.html\n",
      "Đang xử lý file: About_Seoul.html\n",
      "Đang xử lý file: Attractions_Incheon.html\n",
      "Đang xử lý file: Attraction_of_Gyeongju.html\n",
      "Đang xử lý file: Documents_AEMM.html\n",
      "Đang xử lý file: Documents_HRDDM.html\n",
      "Đang xử lý file: Documents_MRT.html\n",
      "Đang xử lý file: Emblem_and_Theme.html\n",
      "Đang xử lý file: Heritage_Gyeongju.html\n",
      "Đang xử lý file: Information_of_Apec.html\n",
      "Đang xử lý file: Introduction_About_Apec_Korea_2025.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "short text: \"Notices\". Defaulting to English.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang xử lý file: Korea_in_Brief.html\n",
      "Đang xử lý file: Local_Eateries_Incheon.html\n",
      "Đang xử lý file: Meetings.html\n",
      "Đang xử lý file: Nature_Culture_Jeju.html\n",
      "Đang xử lý file: Notices.html\n",
      "Đang xử lý file: Practical_Information.html\n",
      "Đang xử lý file: Press_Release_combined.html\n",
      "Cảnh báo: Không tìm thấy div #contents trong file data/crawled_raw_html\\Press_Release_combined.html. Sẽ xử lý toàn bộ HTML.\n",
      "Đang xử lý file: Side_Events.html\n",
      "Đang xử lý file: Themed_Travel_Jeju.html\n",
      "Đang xử lý file: Transportation_Jeju.html\n",
      "Đang xử lý file: Transportation_of_Gyeongju.html\n",
      "\n",
      "--- Đã xử lý 25 file HTML và lưu 261 chunks vào 'data/json_chunks/apec_all_chunks.json' ---\n",
      "\n",
      "--- Chunk mẫu đầu tiên sau khi cải thiện ---\n",
      "{\n",
      "    \"id\": \"f1e9e541-6d4c-4b6f-8bd8-b5d617273523\",\n",
      "    \"topic\": \"About Busan\",\n",
      "    \"sub_topic\": \"N/A\",\n",
      "    \"content\": \"Busan About Busan About Busan\",\n",
      "    \"source_file\": \"About_Busan.html\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def extract_main_content_html(html_file_path):\n",
    "    \"\"\"\n",
    "    Extracts the HTML content from the main content area of the page.\n",
    "    Assumes main content is within <div id=\"contents\">...</div>.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(html_file_path, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: File HTML không tìm thấy tại '{html_file_path}'\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Tìm thẻ div có id=\"contents\"\n",
    "    main_contents_div = soup.find('div', id='contents')\n",
    "    \n",
    "    if main_contents_div:\n",
    "        # Loại bỏ các thẻ không liên quan đến nội dung chính bên trong main_contents_div\n",
    "        # Ví dụ: scripts, styles, các menu con trong sidebar nếu có, pagination, v.v.\n",
    "        for tag in main_contents_div(['script', 'style', 'nav', 'form', 'img', 'svg', 'header', 'footer']):\n",
    "            tag.decompose()\n",
    "        \n",
    "        # Trả về HTML đã được làm sạch của khu vực nội dung chính\n",
    "        return str(main_contents_div)\n",
    "    else:\n",
    "        print(f\"Cảnh báo: Không tìm thấy div #contents trong file {html_file_path}. Sẽ xử lý toàn bộ HTML.\")\n",
    "        return html_content # Trả về toàn bộ HTML nếu không tìm thấy khu vực chính\n",
    "\n",
    "def process_html_files_to_chunks_smartly(html_dir=\"backend/data/crawled_raw_html\", output_json_path=\"backend/data/apec_all_chunks.json\"):\n",
    "    all_chunks_data = []\n",
    "    \n",
    "    output_data_dir = os.path.dirname(output_json_path)\n",
    "    os.makedirs(output_data_dir, exist_ok=True)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len,\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    html_files = glob.glob(os.path.join(html_dir, \"*.html\"))\n",
    "    if not html_files:\n",
    "        print(f\"Không tìm thấy file HTML nào trong thư mục '{html_dir}'. Vui lòng crawl dữ liệu trước.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Tìm thấy {len(html_files)} file HTML để xử lý.\")\n",
    "\n",
    "    for html_file in html_files:\n",
    "        file_name = os.path.basename(html_file)\n",
    "        print(f\"Đang xử lý file: {file_name}\")\n",
    "\n",
    "        try:\n",
    "            # Bước mới: Chỉ lấy phần HTML của khu vực nội dung chính\n",
    "            main_content_html = extract_main_content_html(html_file)\n",
    "            \n",
    "            if main_content_html:\n",
    "                # Tạo một file tạm thời chỉ chứa nội dung chính để UnstructuredHTMLLoader đọc\n",
    "                temp_html_path = f\"{html_file}.temp.html\"\n",
    "                with open(temp_html_path, \"w\", encoding=\"utf-8\") as temp_f:\n",
    "                    temp_f.write(main_content_html)\n",
    "\n",
    "                loader = UnstructuredHTMLLoader(temp_html_path)\n",
    "                documents = loader.load() \n",
    "                \n",
    "                # Xóa file tạm sau khi đã load\n",
    "                os.remove(temp_html_path)\n",
    "\n",
    "                chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "                for chunk in chunks:\n",
    "                    topic_from_filename = file_name.replace(\".html\", \"\").replace(\"_page_\", \" Page \").replace(\"_\", \" \")\n",
    "                    \n",
    "                    # Cố gắng lấy topic/sub_topic từ metadata của UnstructuredHTMLLoader nếu có\n",
    "                    # Unstructured loader tự tạo metadata 'url' từ path file tạm, nên sẽ không phải url gốc\n",
    "                    # Bạn có thể tự ánh xạ lại url gốc nếu cần\n",
    "                    \n",
    "                    # Kiểm tra và làm sạch content một lần nữa nếu cần\n",
    "                    cleaned_content = ' '.join(chunk.page_content.split()) \n",
    "                    \n",
    "                    chunk_data = {\n",
    "                        \"id\": str(uuid.uuid4()),\n",
    "                        \"topic\": chunk.metadata.get(\"category\", topic_from_filename), \n",
    "                        \"sub_topic\": chunk.metadata.get(\"title\", chunk.metadata.get(\"header\", \"N/A\")), \n",
    "                        \"content\": cleaned_content, # Sử dụng nội dung đã làm sạch\n",
    "                        \"source_file\": file_name,\n",
    "                        # \"source_url\": \"URL_GOC_CUA_TRANG_NAY\" # Bạn cần một cách để ánh xạ filename về URL gốc\n",
    "                    }\n",
    "                    all_chunks_data.append(chunk_data)\n",
    "            else:\n",
    "                print(f\"Không tìm thấy nội dung chính để xử lý từ file: {file_name}. Bỏ qua file này.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi khi xử lý file '{html_file}': {e}\")\n",
    "            \n",
    "    with open(output_json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_chunks_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\n--- Đã xử lý {len(html_files)} file HTML và lưu {len(all_chunks_data)} chunks vào '{output_json_path}' ---\")\n",
    "    \n",
    "    if all_chunks_data:\n",
    "        print(\"\\n--- Chunk mẫu đầu tiên sau khi cải thiện ---\")\n",
    "        print(json.dumps(all_chunks_data[0], ensure_ascii=False, indent=4))\n",
    "\n",
    "# Để chạy:\n",
    "if __name__ == \"__main__\":\n",
    "    # Đảm bảo thư mục 'crawled_html' tồn tại và chứa các file HTML\n",
    "    # Bạn có thể chạy script crawl_all_html.py trước\n",
    "    # Vị trí của script này: backend/scripts/process_html_to_chunks_smartly.py\n",
    "    # Thư mục chứa HTML đã crawl: crawled_html (nếu script crawl_all_html.py đặt ngang cấp)\n",
    "    # Hoặc backend/crawled_html (nếu bạn muốn tổ chức bên trong backend)\n",
    "\n",
    "    # Giả sử bạn đang chạy script từ thư mục gốc của dự án\n",
    "    # và thư mục 'crawled_html' nằm ở đó\n",
    "    html_input_directory = \"data/crawled_raw_html\"\n",
    "    output_json_file = \"data/json_chunks/apec_all_chunks.json\"\n",
    "\n",
    "    process_html_files_to_chunks_smartly(html_input_directory, output_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ec851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
